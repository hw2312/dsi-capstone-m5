{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"data_preprocessing.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"nWQew7v_11v-"},"source":["## Modification (DWoo): Loading data files directly from Google Drive"]},{"cell_type":"code","metadata":{"id":"MoccdRvbcYgG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jayrjmeq10aG","executionInfo":{"status":"ok","timestamp":1616867524299,"user_tz":240,"elapsed":25997,"user":{"displayName":"David Woo","photoUrl":"","userId":"06124808101052513949"}},"outputId":"20c5cea5-30a8-4166-e8f6-d5c7c8627cbd"},"source":["import os\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hKoWBwzg2FZB"},"source":["base_dir_path = '/gdrive/MyDrive/Forecasting - DSI Capstone Spring \\'21/Colabs'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BK3RzRAI7xyD"},"source":["# 1. Main setup"]},{"cell_type":"code","metadata":{"id":"UXyksCKE7xyE"},"source":["# General imports\n","import numpy as np\n","import pandas as pd\n","import os, sys, gc, time, warnings, pickle, psutil, random\n","\n","from math import ceil\n","\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wu58eDDj7xyE"},"source":["## Simple \"Memory profilers\" to see memory usage\n","def get_memory_usage():\n","    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n","        \n","def sizeof_fmt(num, suffix='B'):\n","    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n","        if abs(num) < 1024.0:\n","            return \"%3.1f%s%s\" % (num, unit, suffix)\n","        num /= 1024.0\n","    return \"%.1f%s%s\" % (num, 'Yi', suffix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDKnrJWd7xyE"},"source":["## Memory Reducer\n","# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n","# :verbose                                        # type: bool\n","def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2    \n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                       df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)    \n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40Op_13q7xyF"},"source":["## Merging by concat to not lose dtypes\n","def merge_by_concat(df1, df2, merge_on):\n","    merged_gf = df1[merge_on]\n","    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n","    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n","    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n","    return df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeayTVrg7xyF"},"source":["########################### Vars\n","#################################################################################\n","TARGET = 'sales'         # Our main target\n","START_TRAIN = 1069         # Last day in train set\n","END_TRAIN = 1798       # Last day in train set\n","MAIN_INDEX = ['id','d']  # We can identify item by these columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cIArfJU7xyF"},"source":["Sample training date range: 2014-01-01 (d_1069) to 2015-12-31 (d_1798)"]},{"cell_type":"code","metadata":{"id":"SmlrfHJR7xyG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616867892261,"user_tz":240,"elapsed":4497,"user":{"displayName":"David Woo","photoUrl":"","userId":"06124808101052513949"}},"outputId":"89b7650c-a891-43c4-b556-82d7e855b13d"},"source":["########################### Load Data\n","#################################################################################\n","print('Load Main Data')\n","\n","# Here are reafing all our data \n","# without any limitations and dtype modification\n","train_df = pd.read_csv(os.path.join(base_dir_path, 'data/sales_train_validation_ca1.csv'))\n","prices_df = pd.read_csv(os.path.join(base_dir_path, 'data/sell_prices_ca1.csv'))\n","calendar_df = pd.read_csv(os.path.join(base_dir_path, 'data/calendar.csv'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load Main Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-kE4BCHLG2XG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616634512771,"user_tz":240,"elapsed":29729,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"8a4f4ec0-1cf9-4e20-e633-890ebe5afd4a"},"source":["[print(df.shape) for df in [train_df, prices_df, calendar_df]]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3049, 736)\n","(321293, 4)\n","(1969, 14)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[None, None, None]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"9KpWLDj_G_Qs"},"source":["# # Reduce dataset by just looking at one store\n","# train_df = train_df.loc[train_df.store_id == 'CA_1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkP7teBlJn1O"},"source":["def make_grid():\n","  ########################### Make Grid\n","  #################################################################################\n","  print('Create Grid')\n","\n","  # Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n","  # and labels are 'd_' coulmns\n","\n","  index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n","  grid_df = pd.melt(train_df, \n","                    id_vars = index_columns, \n","                    var_name = 'd', \n","                    value_name = TARGET)\n","\n","  print('Train rows:', len(train_df), len(grid_df))\n","\n","  # To be able to make predictions\n","  # we need to add \"test set\" to our grid\n","  add_grid = pd.DataFrame()\n","  for i in range(1,29):\n","      temp_df = train_df[index_columns]\n","      temp_df = temp_df.drop_duplicates()\n","      temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n","      temp_df[TARGET] = np.nan\n","      add_grid = pd.concat([add_grid,temp_df])\n","\n","  grid_df = pd.concat([grid_df,add_grid])\n","  grid_df = grid_df.reset_index(drop=True)\n","\n","  # # Let's check our memory usage\n","  print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n","\n","  # We can free some memory \n","  # by converting \"strings\" to categorical\n","  # it will not affect merging and \n","  # we will not lose any valuable data\n","  for col in index_columns:\n","      grid_df[col] = grid_df[col].astype('category')\n","\n","  # Let's check again memory usage\n","  print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n","\n","  return grid_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkgBJsVYQGev"},"source":["### Store-product level features"]},{"cell_type":"code","metadata":{"id":"DY5zSPOw7xyG"},"source":["def item_release_date(item_grid_df):\n","  ########################### Product Release date\n","  #################################################################################\n","  print('Release week')\n","\n","  # It seems that leadings zero values\n","  # in each train_df item row\n","  # are not real 0 sales but mean\n","  # absence for the item in the store\n","  # we can safe some memory by removing\n","  # such zeros\n","\n","  # Prices are set by week\n","  # so it we will have not very accurate release week \n","  release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n","  release_df.columns = ['store_id','item_id','release']\n","\n","  # Now we can merge release_df\n","  item_grid_df = merge_by_concat(item_grid_df, release_df, ['store_id','item_id'])\n","  del release_df\n","\n","  # We want to remove some \"zeros\" rows\n","  # from item_grid_df \n","  # to do it we need wm_yr_wk column\n","  # let's merge partly calendar_df to have it\n","  item_grid_df = merge_by_concat(item_grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n","                        \n","  # Now we can cutoff some rows \n","  # and safe memory \n","  item_grid_df = item_grid_df[item_grid_df['wm_yr_wk']>=item_grid_df['release']]\n","  item_grid_df = item_grid_df.reset_index(drop=True)\n","\n","  # Let's check our memory usage\n","  print(\"{:>20}: {:>8}\".format('Original item_grid_df',sizeof_fmt(item_grid_df.memory_usage(index=True).sum())))\n","\n","  # Should we keep release week \n","  # as one of the features?\n","  # Only good CV can give the answer.\n","  # Let's minify the release values.\n","  # Min transformation will not help here \n","  # as int16 -> Integer (-32768 to 32767)\n","  # and our item_grid_df['release'].max() serves for int16\n","  # but we have have an idea how to transform \n","  # other columns in case we will need it\n","  item_grid_df['release'] = item_grid_df['release'] - item_grid_df['release'].min()\n","  item_grid_df['release'] = item_grid_df['release'].astype(np.int16)\n","\n","  # Let's check again memory usage\n","  print(\"{:>20}: {:>8}\".format('Reduced item_grid_df',sizeof_fmt(item_grid_df.memory_usage(index=True).sum())))  \n","\n","  return item_grid_df\n","\n","def make_item_price_features():\n","  ########################### Prices\n","  #################################################################################\n","  print('Prices')\n","\n","  price_feature_df = prices_df.copy()\n","\n","  # We can do some basic aggregations\n","  price_feature_df['price_max'] = price_feature_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n","  price_feature_df['price_min'] = price_feature_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n","  price_feature_df['price_std'] = price_feature_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n","  price_feature_df['price_mean'] = price_feature_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n","\n","  # and do price normalization (min/max scaling)\n","  price_feature_df['price_norm'] = price_feature_df['sell_price']/price_feature_df['price_max']\n","\n","  # Some items are can be inflation dependent\n","  # and some items are very \"stable\"\n","\n","  price_feature_df['price_nunique'] = price_feature_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique') \n","  price_feature_df['item_nunique'] = price_feature_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n","\n","  # I would like some \"rolling\" aggregations\n","  # but would like months and years as \"window\"\n","  calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n","  calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk']) # distinct(.keep_all = True)\n","  price_feature_df = price_feature_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n","  del calendar_prices\n","\n","  # Now we can add price \"momentum\" (some sort of)\n","  # Shifted by week \n","  # by month mean\n","  # by year mean\n","  price_feature_df['price_momentum'] = price_feature_df['sell_price']/price_feature_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n","  price_feature_df['price_momentum_m'] = price_feature_df['sell_price']/price_feature_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n","  price_feature_df['price_momentum_y'] = price_feature_df['sell_price']/price_feature_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n","\n","  return price_feature_df.drop(['month', 'year'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPc_ej_F7xyR"},"source":["def make_calendar_features(grid_df):\n","  ########################### Merge calendar\n","  #################################################################################\n","  # grid_df = grid_df[MAIN_INDEX]\n","\n","  # Merge calendar partly\n","  icols = ['date',\n","          'd',\n","          'event_name_1',\n","          'event_type_1',\n","          'event_name_2',\n","          'event_type_2',\n","          'snap_CA']\n","\n","  grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n","\n","  # Minify data\n","  # 'snap_' columns we can convert to bool or int8\n","  icols = ['event_name_1',\n","          'event_type_1',\n","          'event_name_2',\n","          'event_type_2',\n","          'snap_CA']\n","\n","  for col in icols:\n","      grid_df[col] = grid_df[col].astype('category')\n","\n","  # Convert to DateTime\n","  grid_df['date'] = pd.to_datetime(grid_df['date'])\n","\n","  # Make some features from date\n","  grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n","  grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n","  grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n","  grid_df['tm_y'] = grid_df['date'].dt.year\n","  grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n","  grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n","\n","  grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8) \n","  grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n","\n","  # Convert 'd' to int\n","  grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n","\n","  return grid_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3awc6oVKXjAy"},"source":["def make_lag_features(grid_df):\n","  # We need only 'id','d','sales'\n","  # to make lags and rollings\n","  lag_df = grid_df[['id','d','sales']]\n","  SHIFT_DAY = 28\n","\n","  # Lags\n","  # with 28 day shift\n","  start_time = time.time()\n","  print('Create lags')\n","\n","  LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n","  lag_df = lag_df.assign(**{\n","          '{}_lag_{}'.format(col, l): lag_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n","          for l in LAG_DAYS\n","          for col in [TARGET]\n","      })\n","\n","  # Minify lag columns\n","  for col in list(lag_df):\n","      if 'lag' in col:\n","          lag_df[col] = lag_df[col].astype(np.float16)\n","\n","  print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n","\n","  # Rollings\n","  # with 28 day shift\n","  start_time = time.time()\n","  print('Create rolling aggs')\n","\n","  for i in [7,14,30,60,180]:\n","      print('Rolling period:', i)\n","      lag_df['rolling_mean_'+str(i)] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n","      lag_df['rolling_std_'+str(i)]  = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n","\n","  # Rollings\n","  # with sliding shift\n","  for d_shift in [1,7,14]: \n","      print('Shifting period:', d_shift)\n","      for d_window in [7,14,30,60]:\n","          col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n","          lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n","      \n","      \n","  print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n","\n","  return lag_df\n","\n","def make_agg_lag_features(grid_df, groupby_col):\n","  # We need only groupby_col, 'd','sales'\n","  # to make lags and rollings\n","  lag_df = grid_df[[groupby_col, 'd','sales']]\n","  SHIFT_DAY = 28\n","\n","  # Lags\n","  # with 28 day shift\n","  start_time = time.time()\n","  print('Create lags')\n","\n","  LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n","  lag_df = lag_df.assign(**{\n","          '{}_lag_{}'.format(col, l): lag_df.groupby([groupby_col])[col].transform(lambda x: x.shift(l))\n","          for l in LAG_DAYS\n","          for col in [TARGET]\n","      })\n","\n","  # Minify lag columns\n","  for col in list(lag_df):\n","      if 'lag' in col:\n","          lag_df[col] = lag_df[col].astype(np.float16)\n","\n","  print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n","\n","  # Rollings\n","  # with 28 day shift\n","  start_time = time.time()\n","  print('Create rolling aggs')\n","\n","  for i in [7,14,30,60,180]:\n","      print('Rolling period:', i)\n","      lag_df['rolling_mean_'+str(i)] = lag_df.groupby([groupby_col])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n","      lag_df['rolling_std_'+str(i)]  = lag_df.groupby([groupby_col])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n","\n","  # Rollings\n","  # with sliding shift\n","  for d_shift in [1,7,14]: \n","      print('Shifting period:', d_shift)\n","      for d_window in [7,14,30,60]:\n","          col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n","          lag_df[col_name] = lag_df.groupby([groupby_col])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n","      \n","      \n","  print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n","\n","  return lag_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfzhQEB77xyU"},"source":["def make_mean_encoding_feature(grid_df, icols):\n","  ########################### Apply on grid_df\n","  #################################################################################\n","  # lets read grid from \n","  # https://www.kaggle.com/kyakovlev/m5-simple-fe\n","  # to be sure that our grids are aligned by index\n","  \n","  # grid_df['sales'][grid_df['d']>(1941-28)] = np.nan\n","\n","  for col in icols:\n","      print('Encoding', col)\n","      col_name = '_'+'_'.join(col)+'_'\n","      grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)['sales'].transform('mean').astype(np.float16)\n","      grid_df['enc'+col_name+'std'] = grid_df.groupby(col)['sales'].transform('std').astype(np.float16)\n","\n","  return grid_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nu9R6e-0dDp9"},"source":["### Product-store level data"]},{"cell_type":"code","metadata":{"id":"j552qwbS7xyP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616634526119,"user_tz":240,"elapsed":43045,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"6d89e4ef-1764-4362-99e0-a7d189a723af"},"source":["product_grid_df = make_grid()\n","product_grid_df = item_release_date(product_grid_df)\n","price_feature_df = make_item_price_features()\n","\n","product_grid_df = reduce_mem_usage(product_grid_df)\n","price_feature_df = reduce_mem_usage(price_feature_df)\n","\n","# Merge Prices\n","original_columns = list(product_grid_df)\n","product_grid_df = product_grid_df.merge(price_feature_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n","product_grid_df = make_calendar_features(product_grid_df)\n","\n","product_grid_df = reduce_mem_usage(product_grid_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Create Grid\n","Train rows: 3049 2225770\n","    Original grid_df: 141.1MiB\n","     Reduced grid_df:  53.1MiB\n","Release week\n","Original item_grid_df:  85.5MiB\n","Reduced item_grid_df:  72.7MiB\n","Prices\n","Mem. usage decreased to 47.11 Mb (35.2% reduction)\n","Mem. usage decreased to 14.40 Mb (60.8% reduction)\n","Mem. usage decreased to 166.39 Mb (0.0% reduction)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JDyfpf8zZrIi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616634912136,"user_tz":240,"elapsed":429056,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"e363cd75-4b19-4dd4-c9da-13865007bbd8"},"source":["# Create lag features\n","lag_df = make_lag_features(product_grid_df)\n","lag_cols = [col for col in lag_df.columns if col != 'sales']\n","product_grid_df = product_grid_df.merge(lag_df[lag_cols], how='left', on=['id', 'd'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Create lags\n","2.33 min: Lags\n","Create rolling aggs\n","Rolling period: 7\n","Rolling period: 14\n","Rolling period: 30\n","Rolling period: 60\n","Rolling period: 180\n","Shifting period: 1\n","Shifting period: 7\n","Shifting period: 14\n","4.05 min: Lags\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l50xOD_9cAk_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616634918406,"user_tz":240,"elapsed":435316,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"ae434090-98a4-4741-e1fa-620086cafd88"},"source":["product_grid_icols = [\n","  ['store_id'],\n","  ['cat_id'],\n","  ['dept_id'],\n","  ['store_id', 'cat_id'],\n","  ['store_id', 'dept_id'],\n","  ['item_id'],\n","  ['item_id', 'store_id']\n","]\n","product_grid_df = make_mean_encoding_feature(product_grid_df, product_grid_icols)\n","product_grid_df = reduce_mem_usage(product_grid_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoding ['store_id']\n","Encoding ['cat_id']\n","Encoding ['dept_id']\n","Encoding ['store_id', 'cat_id']\n","Encoding ['store_id', 'dept_id']\n","Encoding ['item_id']\n","Encoding ['item_id', 'store_id']\n","Mem. usage decreased to 383.85 Mb (0.0% reduction)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NVXZNVBrzONe"},"source":["# Store-product label encoding\n","dynamic_cat_cols = [col for col in product_grid_df.columns if 'event_' in col or 'tm_' in col]\n","obj_cols = [col for col in dynamic_cat_cols if 'tm_' not in col]\n","tmp_cols = [col for col in dynamic_cat_cols if 'tm_' in col]\n","for col in obj_cols:\n","  le = LabelEncoder()\n","  le.fit(product_grid_df[col].astype(str))\n","  product_grid_df[f'{col}_lencoded'] = le.transform(product_grid_df[col].astype(str))\n","\n","dynamic_cat_cols = tmp_cols + [f'{col}_lencoded' for col in obj_cols]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBIxRcFV985E"},"source":["# Drop the original event columns\n","product_grid_df.drop(columns=obj_cols, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhGpg8oDdIda"},"source":["### Store-level data"]},{"cell_type":"code","metadata":{"id":"QddzBGGxdKIL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616635320257,"user_tz":240,"elapsed":3020,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"5838c80d-80aa-4cc6-b098-eaf74f9a81ed"},"source":["store_grid_df = make_grid()\n","store_grid_df = store_grid_df.groupby(['store_id', 'd']).agg(dict(sales=sum)).reset_index()\n","store_grid_df = make_calendar_features(store_grid_df)\n","store_grid_df = reduce_mem_usage(store_grid_df)\n","\n","# restore prediction horizon back to NaN \n","store_grid_df.loc[(store_grid_df.d>END_TRAIN),'sales'] = np.nan"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Create Grid\n","Train rows: 3049 2225770\n","    Original grid_df: 141.1MiB\n","     Reduced grid_df:  53.1MiB\n","Mem. usage decreased to  0.03 Mb (14.4% reduction)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RWiChxAseAgh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616635323239,"user_tz":240,"elapsed":368,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"74cb1c88-ed71-4d20-a53a-816f6aa2fbdc"},"source":["# Create lag features\n","lag_df = make_agg_lag_features(store_grid_df, 'store_id')\n","lag_cols = [col for col in lag_df.columns if col != 'sales']\n","store_grid_df = store_grid_df.merge(lag_df[lag_cols], how='left', on=['store_id', 'd'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Create lags\n","0.00 min: Lags\n","Create rolling aggs\n","Rolling period: 7\n","Rolling period: 14\n","Rolling period: 30\n","Rolling period: 60\n","Rolling period: 180\n","Shifting period: 1\n","Shifting period: 7\n","Shifting period: 14\n","0.00 min: Lags\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2bCSSUSGeAgy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616635326599,"user_tz":240,"elapsed":214,"user":{"displayName":"David Schemitsch","photoUrl":"","userId":"00391092019980993367"}},"outputId":"a3739ed6-be3e-4dc0-ed12-a6b5200b5823"},"source":["store_grid_icols = [\n","  ['store_id'],\n","]\n","store_grid_df = make_mean_encoding_feature(store_grid_df, store_grid_icols)\n","store_grid_df = reduce_mem_usage(store_grid_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoding ['store_id']\n","Mem. usage decreased to  0.08 Mb (0.0% reduction)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t21lfnT60QXf"},"source":["# Store label encoding\n","dynamic_cat_cols = [col for col in store_grid_df.columns if 'event_' in col or 'tm_' in col]\n","obj_cols = [col for col in dynamic_cat_cols if 'tm_' not in col]\n","tmp_cols = [col for col in dynamic_cat_cols if 'tm_' in col]\n","for col in obj_cols:\n","  le = LabelEncoder()\n","  le.fit(store_grid_df[col].astype(str))\n","  store_grid_df[f'{col}_lencoded'] = le.transform(store_grid_df[col].astype(str))\n","\n","dynamic_cat_cols = tmp_cols + [f'{col}_lencoded' for col in obj_cols]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7_46hhK-oXH"},"source":["# Drop the original event columns\n","store_grid_df.drop(columns=obj_cols, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7ZOw4A8c_aF"},"source":["### Save training data to pickle files"]},{"cell_type":"code","metadata":{"id":"SU3GXtwkcX1Z"},"source":["product_grid_df.to_pickle(os.path.join(base_dir_path, 'preprocessed_data/train_ca1_store_product.pkl'))\n","store_grid_df.to_pickle(os.path.join(base_dir_path, 'preprocessed_data/train_ca1_store.pkl'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwmG0WUiCLaZ"},"source":["### Aggregate test data to store level"]},{"cell_type":"code","metadata":{"id":"pI9To89KCOvi"},"source":["test_df = pd.read_csv(os.path.join(base_dir_path, 'data/sales_train_evaluation_ca1.csv'))\n","\n","d_cols = [col for col in test_df.columns if 'd_' in col]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqQ8lUQKC2bd"},"source":["store_test_df = test_df.groupby('store_id')[d_cols].sum().reset_index()\n","\n","store_test_df = pd.melt(\n","    store_test_df,\n","    id_vars = 'store_id', \n","    var_name = 'd',\n","    value_name = TARGET\n",").merge(\n","    calendar_df[['d', 'date']],\n","    how='left',\n","    on='d'\n",")\n","store_test_df = store_test_df.assign(\n","    d=store_test_df.d.apply(lambda x: int(x[2:]))\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kkrd4EGDEJGj"},"source":["store_test_df.to_pickle(os.path.join(base_dir_path, 'preprocessed_data/test_ca1_store_no_features.pkl'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g2-OKnBO0hGJ"},"source":["### Process test data for store-product level"]},{"cell_type":"code","metadata":{"id":"hSLq5-sO0s2Q"},"source":["product_store_df = pd.melt(\n","    test_df[['item_id', 'store_id'] + d_cols],\n","    id_vars = ['item_id', 'store_id'],\n","    var_name = 'd',\n","    value_name = TARGET\n",").merge(\n","    calendar_df[['d', 'date']],\n","    how='left',\n","    on='d'\n",")\n","\n","product_store_df = product_store_df.assign(\n","    d=product_store_df.d.apply(lambda x: int(x[2:]))\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DIZEGZSn1nvY"},"source":["product_store_df.to_pickle(os.path.join(base_dir_path, 'preprocessed_data/test_ca1_store_product_no_features.pkl'))"],"execution_count":null,"outputs":[]}]}